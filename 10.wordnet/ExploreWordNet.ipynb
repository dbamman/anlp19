{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores WordNet synsets, presenting a simple method for finding in a text all mentions of all hyponyms of a given node in the WordNet hierarchy (e.g., finding all *buildings* in a text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, spacy\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['tagger,ner,parser'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the synsets for a given word.  The synsets here are roughly ordered by frequency of use (in a small tagged dataset), so that more frequent senses occur first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets=wn.synsets('car')\n",
    "for synset in synsets:\n",
    "    print (synset, synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the words/phrases in that synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma in wn.synset(\"car.n.01\").lemmas():\n",
    "    print (lemma.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from http://www.nltk.org/howto/wordnet.html to get *all* of a synset's hyponym/hypernyms\n",
    "hypo = lambda s: s.hyponyms()\n",
    "hyper = lambda s: s.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all of the synsets that are hyponyms of the target synset (*descendents* in the WordNet hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wn.synset(\"car.n.01\").closure(hypo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all of the synsets that are hyperyms (*ancestors* up the tree) of the target synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wn.synset(\"car.n.01\").closure(hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_hypo(synset):\n",
    "    \"\"\" Returns a list of words/phrases that comprise the hyponyms of a synset. \n",
    "    \"\"\"\n",
    "    words=set()\n",
    "    hyponym_synsets=list(synset.closure(hypo))\n",
    "    hyponym_synsets.append(synset)\n",
    "    for synset in hyponym_synsets:\n",
    "        for l in synset.lemmas():\n",
    "            word=l.name()\n",
    "            word=re.sub(\"_\", \" \", word)\n",
    "            words.add(word)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_words_in_hypo(wn.synset(\"car.n.01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_words_in_text(words, spacy_tokens):\n",
    "    \"\"\" For a given set of words, find each instance among a list of tokens already\n",
    "    processed by spacy.  Returns a list of token indexes that match.  (Note this only\n",
    "    identifies single words, not multi-word phrases.)\n",
    "    \"\"\"\n",
    "    all_matches=[]\n",
    "    for idx, token in enumerate(spacy_tokens):\n",
    "        if token.lemma_ in words:\n",
    "            all_matches.append(idx)\n",
    "    return all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_concordance(matches, spacy_tokens, window=3):\n",
    "    \"\"\" For a given set of token indexes, prints out a window of words around each match,\n",
    "    in the style of a concordance.\n",
    "    \"\"\"\n",
    "    \n",
    "    RED=\"\\x1b[31m\"\n",
    "    BLACK=\"\\x1b[0m\"\n",
    "    \n",
    "    spacing=window*10\n",
    "    for match in matches:\n",
    "        start=match-window\n",
    "        end=match+window+1\n",
    "        if start < 0:\n",
    "            start=0\n",
    "        if end > len(spacy_tokens):\n",
    "            end=len(spacy_tokens)\n",
    "        pre=' '.join([token.text for token in spacy_tokens[start:match]])\n",
    "        post=' '.join([token.text for token in spacy_tokens[match+1:end]])\n",
    "        print(\"%s %s%s%s %s\" % (pre.rjust(spacing), RED, spacy_tokens[match].text, BLACK, post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "    \"\"\" Read a text, replacing all whitespace sequences with a single space.\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        return re.sub(\"\\s+\", \" \", file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book=read_text(\"../data/pride_and_prejudice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens=nlp(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_search(synset, spacy_tokens):\n",
    "    \"\"\" This functions searchs through all of the tokens in the spacy_tokens argument to find\n",
    "    any mention of words in the synset or any of its hyponyms.\n",
    "    \"\"\"\n",
    "    targets=get_words_in_hypo(synset)\n",
    "    matches=find_all_words_in_text(targets, spacy_tokens)\n",
    "    print_concordance(matches, spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Let's do a very coarse tagging of a document to find all of the mentions of a specific WordNet synset and all of its hyponyms. Using the functions above, find all of the color terms in *Pride and Prejudice*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Find all of the vehicles mentioned in *Pride and Prejudice*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Find all of the verbs of speaking in *Pride and Prejudice*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Find all of the people in *Pride and Prejudice*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. The methods above all identify *any* mentions of a WordNet synset in a text  -- e.g., every instance of *bank* would be identified as a hit for query bank.n.01 (\"sloping land ...\"), even if its specific word sense in context was the financial institution (or even a verb).  How might we improve on this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
