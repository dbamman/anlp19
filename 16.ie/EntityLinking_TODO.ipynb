{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores named entity disambiguation and entity linking to Wikipedia pages. The notebook explores string simlarity and cosine similarity as features in an entity linking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser'])\n",
    "# workaround if you are getting an error loading the sapcy 'en' module:\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch candidate Wiki pages\n",
    "We will use Wikipedia as our knowledge base. Working with the full Wikipedia database is computationaly intensive and time consuming. In this notebook we will try to correctly link mentions of \"Michael Jordan\" in natural text to the appropriate entity page on Wikipedia. Let's fetch the 3 most popular Wikipedia pages about Michael Jordan: \n",
    "- [Michael Jordan](https://en.wikipedia.org/wiki/Michael_Jordan) (NBA player), \n",
    "- [Micahel B. Jordan](https://en.wikipedia.org/wiki/Michael_B._Jordan) (actor), \n",
    "- [Michael I. Jordan](https://en.wikipedia.org/wiki/Michael_I._Jordan) (UC Berkeley Professor)<br/>\n",
    "\n",
    "This will take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jordan_type = {\n",
    "    'Michael Jordan': 'NBA',\n",
    "    'Michael B. Jordan': 'actor',\n",
    "    'Michael I. Jordan': 'professor'\n",
    "}\n",
    "\n",
    "page_titles = ['Michael Jordan', 'Michael B. Jordan', 'Michael I. Jordan']\n",
    "\n",
    "# Fetch each page, store the title and summary text.\n",
    "pages = []\n",
    "for title in page_titles:\n",
    "    print(\"downloading '{}'\".format(title))\n",
    "    page = wikipedia.page(title)\n",
    "    pages.append((page.title, page.summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "While you are loading the Wikipedia pages into your notebook, let's generate some test data to work with! \n",
    "Find a sentence with a mention of \"Michael Jordan\" online. You can use news headlines, exerpts from news articles, blog posts, etc. Please don't use any content from Wikipedia.<br/>\n",
    "<br/>Open up this [Google sheet](https://docs.google.com/spreadsheets/d/1_IarHbOP4m3DWxjiacFCEIHNOKkSTGdfACO4D8hMqA0/edit#gid=0) and paste your sentence in a new row.\n",
    "\n",
    "Once everyone has populated the Google sheet, download it as a .csv and save it in `../data/MichaelJordan.csv`. Execute the next cell to load all the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \n",
    "    \"\"\" Read data from file \n",
    "    Input: \n",
    "        - filename containing one document per line\n",
    "    Output:\n",
    "        - a list of spaCy tokenized documents\n",
    "    \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            doc = nlp(line.strip())\n",
    "            data.append(doc)\n",
    "    \n",
    "    print(\"loaded {} docs\".format(len(data)))\n",
    "    \n",
    "    return data\n",
    "\n",
    "docs = read_data(\"../data/MichaelJordan.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. String Similarity\n",
    "\n",
    "We can directly compare the string of the mention to the title of each candidate Wikipedia page. \n",
    "\n",
    "TODO: Implement a `string_similarity` function that takes two strings as input and returs a similarity score between 0 and 1, with 1 indicating high similarity and 0 indicating no similarity.\n",
    "\n",
    "You can choose to meausre word-level or character-level similarity. Come up with your own metric, or look into the common string similarity algorithms: [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance), [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index), [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Return a string similarity score betweein 0 and 1, \n",
    "    where 1 indicates exact match, 0 indicates no match\n",
    "    \n",
    "    e.g. string_similarity('hello world', 'hello world') = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your string similarity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_string_similarity(str1, str2):\n",
    "    print(\"'{}' vs. '{}' = {}\".format(str1, str2, string_similarity(str1, str2)))\n",
    "    \n",
    "check_string_similarity(\"hello world\", \"hello world\") # should be 1\n",
    "check_string_similarity(\"Michael B. Jordan\", \"Michael I. Jordan\")\n",
    "check_string_similarity(\"Hillary Clinton\", \"Bill Clinton\")\n",
    "check_string_similarity(\"one two three\", \"four five six\") # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the `string_similarity` function to compare entity mentions with Wikipedia page titles to see which Wikipedia page is the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_string_similarity(docs, pages):\n",
    "    \"\"\"\n",
    "    For each doc return the Wiki page that is the best match based on string similarity\n",
    "    \n",
    "    docs - is a list of documents containing entity mentions that we want to Wikify\n",
    "    pages - is a list of Wikipedia (title, summary) tuples\n",
    "    \n",
    "    Returns\n",
    "    result - a list of (title, score) tuples representing the best Wiki page match for each doc.\n",
    "    (title is the best Wikipedia page title; score is is the string similarity score)\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    for doc in docs:\n",
    "        best_score = -1\n",
    "        best_page = pages[0]\n",
    "        \n",
    "        # find the best matching entity in doc\n",
    "        for e in doc.ents:\n",
    "            # for each entity calculate a strings similarity score with each Wiki page\n",
    "            for page in pages:\n",
    "                sim = string_similarity(e.text, page[0])\n",
    "                # if new best score - store\n",
    "                if sim > best_score:\n",
    "                    best_score = sim\n",
    "                    best_page = page\n",
    "        \n",
    "        result.append((best_page[0], best_score))\n",
    "    return result\n",
    "    \n",
    "def display_result(closest, docs):\n",
    "    \n",
    "    \"\"\" Print results in readable format\"\"\"\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        print(docs[i])\n",
    "        title = closest[i][0]\n",
    "        print(\"{} - {} ({})\".format(title, jordan_type[title], closest[i][1]))\n",
    "        print()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to see how well string similarity performs for Michael Jordan entity linking. Observe that string similarity on its own is not enough for correct entity disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "string_rank = rank_string_similarity(docs, pages)\n",
    "display_result(string_rank, docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Cosine Similarity\n",
    "Now we will implement a more robust measure of similarity between enity mentions and Wikipedia pages by incorporating the context of each mention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Glove word embeddings to generate vector representations of entity context and Wikipedia pages. Lets load them here. \n",
    "\n",
    "You should already have the glove file in your data/ directory. If you get a 'file not found' error, download the top 50K words in the \"Common Crawl (42B)\"  vectors (300-dimensional) here: [glove.42B.300d.50K.txt](https://drive.google.com/file/d/1n1jt0UIdI3CD26cY1EIeks39XH5S8O8M/view?usp=sharing) and place  in your `data` directory. Skip ahead to the next cell to reformat and load the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Load embeddings from file\n",
    "    Returns\n",
    "    - embeddings: a list of embedding vectors\n",
    "    - vocab: a dictionary of {word: idx} pairs where idx points to the index of the 'word' embedding in the embeddings list\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab={}\n",
    "    embeddings=[]\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        embeddings.append(np.zeros(size))  # 0 = padding\n",
    "        embeddings.append(np.zeros(size))  # 1 = UNK,\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            \n",
    "            embeddings.append(val)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    return np.array(embeddings), vocab\n",
    "\n",
    "embeddings, vocab=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just downloaded glove embeddings for the first time, run the cell below. Else, skip ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run after downloading glove.42B.300d.50K.txt  for the first time\n",
    "Only run this cell if you got a 'file not found' error above\n",
    "\"\"\"\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# First we have to convert the Glove format into w2v format; this creates a new file\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)\n",
    "\n",
    "# now load the embeddings\n",
    "embeddings, vocab=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement the `get_doc_representation` function below to return an embedding vector that represents all words in the input document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_representation(doc, vocab, embeddings):\n",
    "    \"\"\"\n",
    "    Return one vector that represents the entire array of input tokens.\n",
    "    Output shape should == embeddings.shape[1]\n",
    "\n",
    "    Input: \n",
    "        - doc: a scpaCy document instance\n",
    "        - vocab: a dictionary of (word, index) pairs. \n",
    "        'index' denotes the location of each word in the 'embeddings' list\n",
    "        - embeddings: a list of word embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(v1, v2):\n",
    "    \"\"\" Returns cosine similarity between two vectors \"\"\"\n",
    "    cos = np.dot(v1, v2) / (np.sqrt(np.dot(v1,v1)) * np.sqrt(np.dot(v2,v2)))\n",
    "    return cos\n",
    "\n",
    "def rank_cos_similarity(docs, pages, vocab, embeddings):\n",
    "    \"\"\"\n",
    "    For each doc return the Wiki page that is the best match based on cosine similarity\n",
    "    \n",
    "    Input:\n",
    "        - docs: a list of documents containing entity mentions that we want to Wikify\n",
    "        - pages: a list of Wikipedia (title, summary) tuples\n",
    "    \n",
    "    Returns\n",
    "        - a list of (title, score) tuples representing the best Wiki page match for each doc.\n",
    "        (title is the best Wikipedia page title; score is is the string similarity score)\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # get page representations\n",
    "    page_representations = []\n",
    "    for p in pages:\n",
    "        summary = p[1]\n",
    "        emb = get_doc_representation(summary.split(\" \"), vocab, embeddings)\n",
    "        page_representations.append((p[0], emb))\n",
    "    \n",
    "    for doc in docs:     \n",
    "        doc_representation = get_doc_representation([token.text for token in doc], vocab, embeddings)\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for p in page_representations:\n",
    "            title = p[0]\n",
    "            emb = p[1]\n",
    "            sim = cos_similarity(doc_representation, emb)\n",
    "            scores.append((p[0], sim))\n",
    "\n",
    "        scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        result.append(scores[0])\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to see how well cosine similarity performs for Michael Jordan entity linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_cos = rank_cos_similarity(docs, pages, vocab, embeddings)\n",
    "display_result(closest_cos, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Other Features?\n",
    "\n",
    "Based on your results above, can you think of other features that might help improve entity linking performance further? Name one feature and justify why you think it is a good feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your response to Q3 here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
