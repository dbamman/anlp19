{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thie notebook explores the multilayer perceptron for binary text classification for your text classification problem, using the keras library.  Before starting, be sure to install the following versions of tensorflow and keras (for python 3.7):\n",
    "\n",
    "```sh\n",
    "pip install tensorflow==1.13.0-rc2\n",
    "pip install keras==2.2.4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from random import choices\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx,line in enumerate(file):\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)\n",
    "testX, testY=read_data(\"%s/test.tsv\" % directory)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=True, strip_accents=None, binary=True)\n",
    "X_train = vectorizer.fit_transform(trainX)\n",
    "X_dev = vectorizer.transform(devX)\n",
    "X_test = vectorizer.transform(testX)\n",
    "\n",
    "_,vocabSize=X_train.shape\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "\n",
    "Y_train=le.transform(trainY)\n",
    "Y_dev=le.transform(devY)\n",
    "Y_test=le.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(vocabSize,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, verbose=1, epochs=10):\n",
    "    if verbose > 0:\n",
    "        print (model.summary())\n",
    "\n",
    "    # stop training early when the val_loss on the dev data stops going down\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=5,\n",
    "        verbose=0, \n",
    "        mode='auto')\n",
    "\n",
    "    # when training, save the model that has the best val_loss on the dev data\n",
    "    modelName=\"mymodel.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "    # train\n",
    "    model.fit(X_train, Y_train, \n",
    "                validation_data=(X_dev, Y_dev),\n",
    "                epochs=epochs,\n",
    "                verbose=verbose,\n",
    "                callbacks=[checkpoint, early_stopping])\n",
    "    \n",
    "    # load best trained model (from checkpoint)\n",
    "    model.load_weights(modelName)\n",
    "\n",
    "    dev_loss, dev_accuracy = model.evaluate(X_dev, Y_dev, batch_size=128, verbose=verbose)\n",
    "    if verbose > 0:\n",
    "        print(\"Dev Accuracy: %.3f\" % dev_accuracy)\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, Y_test, batch_size=128, verbose=verbose)\n",
    "    if verbose > 0:\n",
    "        print(\"Test Accuracy: %.3f\" % test_accuracy)\n",
    "\n",
    "    return dev_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Experiment with the network structure that works best for your binary classification dataset.  Explore the following choices:**\n",
    "- a) number of layers in the MLP\n",
    "- b) the size of each layer\n",
    "- c) the activation functions\n",
    "- d) the use of dropout  <br><br>\n",
    "**Create 5 different models and execute them below.  One of the models should be logistic regression (i.e., an MLP with no hidden layers).<br><br>**\n",
    "**Which architecture performs best on the development data?  <br>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model1():\n",
    "    model = Sequential()\n",
    "    # your model here\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model1=get_model1()\n",
    "dev_accuracy, test_accuracy=train_and_evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model2():\n",
    "    model = Sequential()\n",
    "    # your model here\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model2=get_model2()\n",
    "dev_accuracy, test_accuracy=train_and_evaluate(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model3():\n",
    "    model = Sequential()\n",
    "    # your model here\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model3=get_model3()\n",
    "dev_accuracy, test_accuracy=train_and_evaluate(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model4():\n",
    "    model = Sequential()\n",
    "    # your model here\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model4=get_model4()\n",
    "dev_accuracy, test_accuracy=train_and_evaluate(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logreg():\n",
    "    model = Sequential()\n",
    "    # your model here\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model\n",
    "    \n",
    "logreg=get_logreg()\n",
    "dev_accuracy, test_accuracy=train_and_evaluate(logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate predictions for a given test set with the `predict_classes` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=logreg\n",
    "predictions=model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: For the single best model you identified in Q1 above, calculate 95% confidence intervals it makes on the test data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: Unlike logistic/linear regression, neural networks converge to different solutions as a function of their *initialization* (the random choice of the initial values for parameters).  For the best model that's not logistic regression you identified in Q1 above, train the model 10 times and save the accuracies attained on the development data.  Plot the distribution of dev accuracies using [pandas.DataFrame.plot.density](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.density.html).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
