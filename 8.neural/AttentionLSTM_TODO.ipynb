{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of a bidirectional LSTM with attention for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Input, Embedding, Lambda, Layer, Multiply, Dropout, Dot, Bidirectional, LSTM\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from math import sqrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    vocab={}\n",
    "    embeddings=[]\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        embeddings.append(np.zeros(size))  # 0 = 0 padding if needed\n",
    "        embeddings.append(np.zeros(size))  # 1 = UNK\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            \n",
    "            embeddings.append(val)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    return np.array(embeddings), vocab, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            # assumes text is already tokenized\n",
    "            text=cols[1].split(\" \")\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(docs, vocab, max_length=200):\n",
    "    \n",
    "    doc_ids=[]\n",
    "    \n",
    "    for doc in docs:\n",
    "        wids=[]\n",
    "        for token in doc[:max_length]:\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            wids.append(val)\n",
    "        \n",
    "        # pad each document to constant width\n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't downloaded the glove vectors, do so first -- the top 50K words in the \"Common Crawl (42B)\"  vectors (300-dimensional) can be found here: [glove.42B.300d.50K.txt](https://drive.google.com/file/d/1n1jt0UIdI3CD26cY1EIeks39XH5S8O8M/view?usp=sharing); download it and place  in your `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab, embedding_size=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)\n",
    "testText, testY=read_data(\"%s/test.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "devX = get_word_ids(devText, vocab, max_length=200)\n",
    "testX = get_word_ids(testText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "Y_dev=np.array(le.transform(devY))\n",
    "Y_test=np.array(le.transform(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayerMasking(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(AttentionLayerMasking, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_embedding_dim=input_shape[-1]\n",
    "        \n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                            shape=(input_embedding_dim,1),\n",
    "                            initializer='uniform',\n",
    "                            trainable=True)\n",
    "        super(AttentionLayerMasking, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        # dot product \n",
    "        x=K.dot(x, self.kernel)\n",
    "        # exponentiate\n",
    "        x=K.exp(x)\n",
    "        \n",
    "        # zero out elements that are masked\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask, axis=-1)\n",
    "            x = x * mask\n",
    "        \n",
    "        # normalize by sum\n",
    "        x /= K.sum(x, axis=1, keepdims=True)\n",
    "        x=K.squeeze(x, axis=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Implement a BiLSTM with attention. Feel free to base your code on the models in Attention.ipynb and LSTM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_with_attention_masking(embeddings, lstm_size=25, dropout_rate=0.25):\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_attention_model=get_bilstm_with_attention_masking(embeddings, lstm_size=25, dropout_rate=0.25)\n",
    "print (bilstm_attention_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=bilstm_attention_model\n",
    "\n",
    "modelName=\"bilstm_attention_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "model.fit(trainX, Y_train, \n",
    "            validation_data=(devX, Y_dev),\n",
    "            epochs=30, batch_size=128,\n",
    "            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the accuracy of your model on the test data?  Report the accuracy score with 95% confidence intervals.  Feel free to use the dev data for model selection (e.g., to hyperparameter choices like the size of hidden LSTM state, etc.), but be careful not to use the test data for this.  See keras [model.predict](https://keras.io/models/model/#predict) to generate predictions for a trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Take the sentence \"I do not like this movie.\" How is representing this sentence by using attention over the individual word embeddings different from representing it with attention over the output of each time step in an bidirectional LSTM?  What information does the LSTM output encode that individual word embeddings don't have access to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
